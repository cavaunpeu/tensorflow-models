{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers.dataset import TensorFlowDataset, import_zip_file\n",
    "from helpers.evaluate import TensorFlowModelEvaluator\n",
    "from models.seq_to_seq import SequenceToSequenceLSTM\n",
    "from vanilla_neural_nets.recurrent_neural_network.training_data import WordLevelRNNTrainingDataBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following network compiles, runs, predicts. However, its predictions are garbage. This is likely because:\n",
    "\n",
    "- I'm embedding a 30-dimensional token space into a higher dimensional space at the head of the network - simply so as to avoid modifying the other RNN/LSTM objects included in this repository.\n",
    "- I likely need to train on a lot more data for many more epochs.\n",
    "\n",
    "For a real application, I'll be using the objects in TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = '../data/text8.zip'\n",
    "BATCH_SIZE = 128\n",
    "ENCODER_TIME_STEPS = 10\n",
    "DECODER_TIME_STEPS = ENCODER_TIME_STEPS + 2\n",
    "TRAINING_EXAMPLES = BATCH_SIZE * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create token lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = list(string.ascii_lowercase) + [' ', 'START_OUTPUT', 'END_OUTPUT', 'PAD']\n",
    "token_to_index_lookup = {token: index for index, token in enumerate(tokens)}\n",
    "index_to_token_lookup = {index: token for token, index in token_to_index_lookup.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = import_zip_file(path=PATH, n_characters=1000000)\n",
    "corpus_as_indices = [token_to_index_lookup[token] for token in list(corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training data and labels, where:\n",
    "    - Training labels are the \"flip\" of training data\n",
    "    - Training labels are prefixed with `START_OUTPUT`, and suffixed with `END_OUTPUT`, for use by our decoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = np.array(corpus_as_indices[:TRAINING_EXAMPLES*(ENCODER_TIME_STEPS)])\n",
    "training_data = training_data.reshape(TRAINING_EXAMPLES, ENCODER_TIME_STEPS)\n",
    "\n",
    "training_labels = np.fliplr(training_data)\n",
    "start_output_tokens = [token_to_index_lookup['START_OUTPUT']] * len(training_labels)\n",
    "end_output_tokens = [token_to_index_lookup['END_OUTPUT']] * len(training_labels)\n",
    "training_labels = np.c_[start_output_tokens, training_labels, end_output_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_dataset = TensorFlowDataset(\n",
    "    data=training_data[:BATCH_SIZE*48], \n",
    "    labels=training_labels[:BATCH_SIZE*48]\n",
    ")\n",
    "\n",
    "validation_dataset = TensorFlowDataset(\n",
    "    data=training_data[BATCH_SIZE*48:BATCH_SIZE*49], \n",
    "    labels=training_labels[BATCH_SIZE*48:BATCH_SIZE*49]\n",
    ")\n",
    "\n",
    "test_dataset = TensorFlowDataset(\n",
    "    data=training_data[BATCH_SIZE*49:], \n",
    "    labels=training_labels[BATCH_SIZE*49:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_LAYER_SIZE = 128\n",
    "HIDDEN_STATE_SIZE = 100\n",
    "LEARNING_RATE = .2\n",
    "N_EPOCHS = int(1e9)\n",
    "N_CLASSES = VOCABULARY_SIZE = len(tokens)\n",
    "\n",
    "START_OUTPUT_INDEX = token_to_index_lookup['START_OUTPUT']\n",
    "END_OUTPUT_INDEX = token_to_index_lookup['END_OUTPUT']\n",
    "PAD_INDEX = token_to_index_lookup['PAD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    dataset = TensorFlowDataset(\n",
    "        data=tf.placeholder(dtype=tf.int32, shape=[None, ENCODER_TIME_STEPS]),\n",
    "        labels=tf.placeholder(dtype=tf.int32, shape=[None, DECODER_TIME_STEPS]),\n",
    "    )\n",
    "    \n",
    "    model = SequenceToSequenceLSTM(\n",
    "        dataset=dataset, \n",
    "        n_classes=N_CLASSES,\n",
    "        embedding_layer_size=EMBEDDING_LAYER_SIZE,\n",
    "        hidden_state_size=HIDDEN_STATE_SIZE,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a padding function in order to generate sequences\n",
    "- When generating sequences, our test data will always be of size `(1, DECODER_TIME_STEPS)`. \n",
    "- To generate new data, we first feed our decoder network the `START_OUTPUT_TOKEN` to predict the second token, then feed the first and second token to predict the third token, then feed the first and second and third token to predict the fourth token, etc. As such, our decoder inputs will be of size `(1, 1)`, `(1, 2)`, `(1, 3)`, etc., respectively.\n",
    "- Because our decoder data placeholder requires input with shape `(None, DECODER_TIME_STEPS)`, we must right-*pad* our decoder input with the `PAD_INDEX` so as to ensure that our input always has shape `(1, DECODER_TIME_STEPS)`.\n",
    "- In each step, we will feed the full, padded, sequence of shape `(1, DECODER_TIME_STEPS)` through the decoder, then pluck the predictions corresponding to the right-most non-`PAD_INDEX` token. We then feed this into a multinomial generator to create a hard prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_predicted_sentence(predicted_sentence, pad_token=PAD_INDEX, decoder_time_steps=DECODER_TIME_STEPS):\n",
    "    return predicted_sentence + (DECODER_TIME_STEPS - len(predicted_sentence))*[PAD_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 3.891\n",
      "Validation Loss: 3.929\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 3.810\n",
      "Validation Loss: 3.836\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 3.704\n",
      "Validation Loss: 3.757\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 3.685\n",
      "Validation Loss: 3.688\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 3.601\n",
      "Validation Loss: 3.636\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 3.582\n",
      "Validation Loss: 3.588\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 3.527\n",
      "Validation Loss: 3.557\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 3.464\n",
      "Validation Loss: 3.530\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 3.439\n",
      "Validation Loss: 3.497\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 3.450\n",
      "Validation Loss: 3.486\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 3.412\n",
      "Validation Loss: 3.468\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 3.399\n",
      "Validation Loss: 3.425\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 3.379\n",
      "Validation Loss: 3.406\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 3.424\n",
      "Validation Loss: 3.411\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 3.343\n",
      "Validation Loss: 3.390\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 3.329\n",
      "Validation Loss: 3.375\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 3.301\n",
      "Validation Loss: 3.340\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 3.249\n",
      "Validation Loss: 3.310\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 3.251\n",
      "Validation Loss: 3.290\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 3.265\n",
      "Validation Loss: 3.303\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 3.258\n",
      "Validation Loss: 3.273\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 3.221\n",
      "Validation Loss: 3.257\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 3.161\n",
      "Validation Loss: 3.244\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 3.163\n",
      "Validation Loss: 3.222\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 3.175\n",
      "Validation Loss: 3.205\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 3.147\n",
      "Validation Loss: 3.204\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 3.124\n",
      "Validation Loss: 3.187\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 3.079\n",
      "Validation Loss: 3.172\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 3.129\n",
      "Validation Loss: 3.156\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 3.106\n",
      "Validation Loss: 3.140\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 3.080\n",
      "Validation Loss: 3.126\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 3.071\n",
      "Validation Loss: 3.120\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 3.093\n",
      "Validation Loss: 3.108\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 3.042\n",
      "Validation Loss: 3.099\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 3.096\n",
      "Validation Loss: 3.089\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 3.043\n",
      "Validation Loss: 3.074\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 3.025\n",
      "Validation Loss: 3.067\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 2.993\n",
      "Validation Loss: 3.075\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 2.990\n",
      "Validation Loss: 3.061\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 3.012\n",
      "Validation Loss: 3.045\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 2.949\n",
      "Validation Loss: 3.038\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 2.994\n",
      "Validation Loss: 3.031\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 2.953\n",
      "Validation Loss: 3.028\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 2.997\n",
      "Validation Loss: 3.031\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 2.981\n",
      "Validation Loss: 3.015\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 2.938\n",
      "Validation Loss: 3.012\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 2.979\n",
      "Validation Loss: 3.014\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 2.969\n",
      "Validation Loss: 3.002\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 2.964\n",
      "Validation Loss: 3.000\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 2.931\n",
      "Validation Loss: 2.986\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 2.956\n",
      "Validation Loss: 2.978\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 2.918\n",
      "Validation Loss: 2.980\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 2.867\n",
      "Validation Loss: 2.973\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 2.959\n",
      "Validation Loss: 2.988\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 2.919\n",
      "Validation Loss: 2.980\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 2.893\n",
      "Validation Loss: 2.975\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 2.932\n",
      "Validation Loss: 2.965\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 2.903\n",
      "Validation Loss: 2.956\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 2.898\n",
      "Validation Loss: 2.952\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 2.932\n",
      "Validation Loss: 2.943\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 2.919\n",
      "Validation Loss: 2.938\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 2.851\n",
      "Validation Loss: 2.931\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 2.818\n",
      "Validation Loss: 2.927\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 2.870\n",
      "Validation Loss: 2.927\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 2.842\n",
      "Validation Loss: 2.925\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 2.901\n",
      "Validation Loss: 2.922\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 2.919\n",
      "Validation Loss: 2.929\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 2.866\n",
      "Validation Loss: 2.921\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 2.887\n",
      "Validation Loss: 2.916\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 2.926\n",
      "Validation Loss: 2.911\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 2.834\n",
      "Validation Loss: 2.907\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 2.880\n",
      "Validation Loss: 2.903\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 2.857\n",
      "Validation Loss: 2.913\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 2.861\n",
      "Validation Loss: 2.897\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 2.873\n",
      "Validation Loss: 2.892\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 2.883\n",
      "Validation Loss: 2.887\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 2.794\n",
      "Validation Loss: 2.894\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 2.855\n",
      "Validation Loss: 2.887\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 2.843\n",
      "Validation Loss: 2.882\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 2.840\n",
      "Validation Loss: 2.891\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 2.818\n",
      "Validation Loss: 2.908\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 2.848\n",
      "Validation Loss: 2.904\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 2.800\n",
      "Validation Loss: 2.886\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 2.814\n",
      "Validation Loss: 2.883\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 2.831\n",
      "Validation Loss: 2.880\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 2.828\n",
      "Validation Loss: 2.880\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 2.831\n",
      "Validation Loss: 2.878\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 2.846\n",
      "Validation Loss: 2.873\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 2.806\n",
      "Validation Loss: 2.874\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 2.859\n",
      "Validation Loss: 2.871\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 2.865\n",
      "Validation Loss: 2.866\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 2.837\n",
      "Validation Loss: 2.862\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 2.811\n",
      "Validation Loss: 2.859\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 2.803\n",
      "Validation Loss: 2.856\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 2.801\n",
      "Validation Loss: 2.852\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 2.772\n",
      "Validation Loss: 2.851\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 2.806\n",
      "Validation Loss: 2.849\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 2.792\n",
      "Validation Loss: 2.853\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 2.792\n",
      "Validation Loss: 2.851\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 2.813\n",
      "Validation Loss: 2.851\n",
      "\n",
      "Test Loss: 2.865\n",
      "\n",
      "Test Sentence: peer relat\n",
      "Predicted Reversal: c vmmrx k \n",
      "\n",
      "Test Sentence: ionships a\n",
      "Predicted Reversal: nys ecoua \n",
      "\n",
      "Test Sentence: ppropriate\n",
      "Predicted Reversal: ehtse et x\n",
      "\n",
      "Test Sentence:  to develo\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: pmental le\n",
      "Predicted Reversal: xnoeb\n",
      "\n",
      "Test Sentence: vel a lack\n",
      "Predicted Reversal: va rtnneey\n",
      "\n",
      "Test Sentence:  of sponta\n",
      "Predicted Reversal: tsic yonoa\n",
      "\n",
      "Test Sentence: neous seek\n",
      "Predicted Reversal:  f \n",
      "\n",
      "Test Sentence: ing to sha\n",
      "Predicted Reversal: kf \n",
      "\n",
      "Test Sentence: re enjoyme\n",
      "Predicted Reversal: nas semrsi\n",
      "\n",
      "Test Sentence: nt interes\n",
      "Predicted Reversal: ht\n",
      "\n",
      "Test Sentence: ts or achi\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: evements w\n",
      "Predicted Reversal: stows\n",
      "\n",
      "Test Sentence: ith other \n",
      "Predicted Reversal: l t eeofio\n",
      "\n",
      "Test Sentence: people e g\n",
      "Predicted Reversal: h\n",
      "\n",
      "Test Sentence:  by a lack\n",
      "Predicted Reversal:  dno ewper\n",
      "\n",
      "Test Sentence:  of showin\n",
      "Predicted Reversal: o svfestrr\n",
      "\n",
      "Test Sentence: g bringing\n",
      "Predicted Reversal: cei\n",
      "\n",
      "Test Sentence:  or pointi\n",
      "Predicted Reversal: dlpveulet\n",
      "\n",
      "Test Sentence: ng out obj\n",
      "Predicted Reversal: nevucieehn\n",
      "\n",
      "Test Sentence: ects of in\n",
      "Predicted Reversal: rr\n",
      "\n",
      "Test Sentence: terest lac\n",
      "Predicted Reversal: irs\n",
      "\n",
      "Test Sentence: k of socia\n",
      "Predicted Reversal:  gdomosic \n",
      "\n",
      "Test Sentence: l or emoti\n",
      "Predicted Reversal: woc tnaht\n",
      "\n",
      "Test Sentence: onal recip\n",
      "Predicted Reversal: x o yiht k\n",
      "\n",
      "Test Sentence: rocity qua\n",
      "Predicted Reversal: fozo eatan\n",
      "\n",
      "Test Sentence: litative i\n",
      "Predicted Reversal: hteaoy\n",
      "\n",
      "Test Sentence: mpairments\n",
      "Predicted Reversal: tl\n",
      "\n",
      "Test Sentence:  in commun\n",
      "Predicted Reversal: nij ykzp a\n",
      "\n",
      "Test Sentence: ication as\n",
      "Predicted Reversal: sonaminio\n",
      "\n",
      "Test Sentence:  manifeste\n",
      "Predicted Reversal: of ht vooj\n",
      "\n",
      "Test Sentence: d by at le\n",
      "Predicted Reversal: mol\n",
      "\n",
      "Test Sentence: ast one of\n",
      "Predicted Reversal: nql dathn\n",
      "\n",
      "Test Sentence:  the follo\n",
      "Predicted Reversal: srs asvcct\n",
      "\n",
      "Test Sentence: wing delay\n",
      "Predicted Reversal: slh\n",
      "\n",
      "Test Sentence:  in or tot\n",
      "Predicted Reversal: ajpieaf\n",
      "\n",
      "Test Sentence: al lack of\n",
      "Predicted Reversal: oxivhts ym\n",
      "\n",
      "Test Sentence:  the devel\n",
      "Predicted Reversal: sb\n",
      "\n",
      "Test Sentence: opment of \n",
      "Predicted Reversal: nn natoot \n",
      "\n",
      "Test Sentence: spoken lan\n",
      "Predicted Reversal: si\n",
      "\n",
      "Test Sentence: guage not \n",
      "Predicted Reversal: saoc opn\n",
      "\n",
      "Test Sentence: accompanie\n",
      "Predicted Reversal: tes \n",
      "\n",
      "Test Sentence: d by an at\n",
      "Predicted Reversal: wf\n",
      "\n",
      "Test Sentence: tempt to c\n",
      "Predicted Reversal: eennc  swy\n",
      "\n",
      "Test Sentence: ompensate \n",
      "Predicted Reversal: sovhsrkto\n",
      "\n",
      "Test Sentence: through al\n",
      "Predicted Reversal: x rhxnwita\n",
      "\n",
      "Test Sentence: ternative \n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: modes of c\n",
      "Predicted Reversal: o drerlaho\n",
      "\n",
      "Test Sentence: ommunicati\n",
      "Predicted Reversal: nohtifyr\n",
      "\n",
      "Test Sentence: on such as\n",
      "Predicted Reversal: iko eo\n",
      "\n",
      "Test Sentence:  gesture o\n",
      "Predicted Reversal: qce\n",
      "\n",
      "Test Sentence: r mime in \n",
      "Predicted Reversal: x s y\n",
      "\n",
      "Test Sentence: individual\n",
      "Predicted Reversal: d\n",
      "\n",
      "Test Sentence: s with ade\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: quate spee\n",
      "Predicted Reversal: tsiexosiht\n",
      "\n",
      "Test Sentence: ch marked \n",
      "Predicted Reversal: bemefefm l\n",
      "\n",
      "Test Sentence: impairment\n",
      "Predicted Reversal: wejf  o\n",
      "\n",
      "Test Sentence:  in the ab\n",
      "Predicted Reversal: f e sc\n",
      "\n",
      "Test Sentence: ility to i\n",
      "Predicted Reversal: u t sniwrj\n",
      "\n",
      "Test Sentence: nitiate or\n",
      "Predicted Reversal: yt\n",
      "\n",
      "Test Sentence:  sustain a\n",
      "Predicted Reversal: g enatsn s\n",
      "\n",
      "Test Sentence:  conversat\n",
      "Predicted Reversal: auo\n",
      "\n",
      "Test Sentence: ion with o\n",
      "Predicted Reversal: recaixanin\n",
      "\n",
      "Test Sentence: thers ster\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: eotyped an\n",
      "Predicted Reversal: p\n",
      "\n",
      "Test Sentence: d repetiti\n",
      "Predicted Reversal: hsoesght c\n",
      "\n",
      "Test Sentence: ve use of \n",
      "Predicted Reversal: jon shsufe\n",
      "\n",
      "Test Sentence: language o\n",
      "Predicted Reversal: niiwardsar\n",
      "\n",
      "Test Sentence: r idiosync\n",
      "Predicted Reversal: am\n",
      "\n",
      "Test Sentence: ratic lang\n",
      "Predicted Reversal: v\n",
      "\n",
      "Test Sentence: uage lack \n",
      "Predicted Reversal: rea r\n",
      "\n",
      "Test Sentence: of varied \n",
      "Predicted Reversal: eet\n",
      "\n",
      "Test Sentence: spontaneou\n",
      "Predicted Reversal: ndhcrvsof \n",
      "\n",
      "Test Sentence: s make bel\n",
      "Predicted Reversal: oh rs hcre\n",
      "\n",
      "Test Sentence: ieve play \n",
      "Predicted Reversal: rautocr eo\n",
      "\n",
      "Test Sentence: or social \n",
      "Predicted Reversal: en pa\n",
      "\n",
      "Test Sentence: imitative \n",
      "Predicted Reversal: yt\n",
      "\n",
      "Test Sentence: play appro\n",
      "Predicted Reversal: f zc twre\n",
      "\n",
      "Test Sentence: priate to \n",
      "Predicted Reversal: yl ep r oh\n",
      "\n",
      "Test Sentence: developmen\n",
      "Predicted Reversal: o u so lfa\n",
      "\n",
      "Test Sentence: tal level \n",
      "Predicted Reversal: iht\n",
      "\n",
      "Test Sentence: restricted\n",
      "Predicted Reversal: aeyecrht h\n",
      "\n",
      "Test Sentence:  repetitiv\n",
      "Predicted Reversal: s ht \n",
      "\n",
      "Test Sentence: e and ster\n",
      "Predicted Reversal: itnh svd\n",
      "\n",
      "Test Sentence: eotyped pa\n",
      "Predicted Reversal: peg a \n",
      "\n",
      "Test Sentence: tterns of \n",
      "Predicted Reversal: tz t\n",
      "\n",
      "Test Sentence: behavior i\n",
      "Predicted Reversal: oc b\n",
      "\n",
      "Test Sentence: nterests a\n",
      "Predicted Reversal: no gnda\n",
      "\n",
      "Test Sentence: nd activit\n",
      "Predicted Reversal: tsap\n",
      "\n",
      "Test Sentence: ies as man\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: ifested by\n",
      "Predicted Reversal: ht  tfi\n",
      "\n",
      "Test Sentence:  at least \n",
      "Predicted Reversal: c hx kfeye\n",
      "\n",
      "Test Sentence: one of the\n",
      "Predicted Reversal: te\n",
      "\n",
      "Test Sentence:  following\n",
      "Predicted Reversal: vdp\n",
      "\n",
      "Test Sentence:  encompass\n",
      "Predicted Reversal: ujfnh otr \n",
      "\n",
      "Test Sentence: ing preocc\n",
      "Predicted Reversal: sbnaw\n",
      "\n",
      "Test Sentence: upation wi\n",
      "Predicted Reversal: p rknaipe\n",
      "\n",
      "Test Sentence: th one or \n",
      "Predicted Reversal: icosndmm\n",
      "\n",
      "Test Sentence: more stere\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: otyped and\n",
      "Predicted Reversal: s\n",
      "\n",
      "Test Sentence:  restricte\n",
      "Predicted Reversal: tyeots t h\n",
      "\n",
      "Test Sentence: d patterns\n",
      "Predicted Reversal: k\n",
      "\n",
      "Test Sentence:  of intere\n",
      "Predicted Reversal: vua  sfi\n",
      "\n",
      "Test Sentence: st that is\n",
      "Predicted Reversal: ybcivtauta\n",
      "\n",
      "Test Sentence:  abnormal \n",
      "Predicted Reversal:  eht uav e\n",
      "\n",
      "Test Sentence: either in \n",
      "Predicted Reversal: v\n",
      "\n",
      "Test Sentence: intensity \n",
      "Predicted Reversal:  ekzhkvsax\n",
      "\n",
      "Test Sentence: or focus a\n",
      "Predicted Reversal: ht  d\n",
      "\n",
      "Test Sentence: pparently \n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: inflexible\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence:  adherence\n",
      "Predicted Reversal: tnuhrfovoj\n",
      "\n",
      "Test Sentence:  to specif\n",
      "Predicted Reversal:  e\n",
      "\n",
      "Test Sentence: ic nonfunc\n",
      "Predicted Reversal: s hc lj lh\n",
      "\n",
      "Test Sentence: tional rou\n",
      "Predicted Reversal: ts f\n",
      "\n",
      "Test Sentence: tines or r\n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: ituals ste\n",
      "Predicted Reversal: gipslrsi y\n",
      "\n",
      "Test Sentence: reotyped a\n",
      "Predicted Reversal: neoc\n",
      "\n",
      "Test Sentence: nd repetit\n",
      "Predicted Reversal: ucemsprylx\n",
      "\n",
      "Test Sentence: ive motor \n",
      "Predicted Reversal: tadiesienn\n",
      "\n",
      "Test Sentence: mannerisms\n",
      "Predicted Reversal: uoh a grnu\n",
      "\n",
      "Test Sentence:  e g hand \n",
      "Predicted Reversal: \n",
      "\n",
      "Test Sentence: or finger \n",
      "Predicted Reversal: rpfzenikny\n",
      "\n",
      "Test Sentence: flapping o\n",
      "Predicted Reversal: g tewt\n",
      "\n",
      "Test Sentence: r twisting\n",
      "Predicted Reversal: t on numo\n",
      "\n",
      "Test Sentence:  or comple\n",
      "Predicted Reversal: a whcramao\n",
      "\n",
      "Test Sentence: x whole bo\n",
      "Predicted Reversal: t on\n",
      "\n",
      "Test Sentence: dy movemen\n",
      "Predicted Reversal: vt ycne\n",
      "\n",
      "Test Sentence: ts persist\n",
      "Predicted Reversal: nific sih \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluator = TensorFlowModelEvaluator(\n",
    "        model=model,\n",
    "        session=session,\n",
    "        validation_dataset=validation_dataset,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        if epoch % int(1e7) == 0:\n",
    "\n",
    "            mini_batch_data, mini_batch_labels = training_dataset.sample(BATCH_SIZE)\n",
    "            mini_batch_dataset = TensorFlowDataset(data=mini_batch_data, labels=mini_batch_labels)\n",
    "            evaluator.optimize(mini_batch_dataset)\n",
    "\n",
    "            print('Epoch: {}'.format( int(epoch / 1e7) ))\n",
    "            print('Train Loss: {:.3f}'.format(evaluator.training_loss))\n",
    "            print('Validation Loss: {:.3f}\\n'.format(evaluator.validation_loss))\n",
    "\n",
    "    print('Test Loss: {0:.3f}\\n'.format(evaluator.test_loss))\n",
    "    \n",
    "    # Predict on test data\n",
    "    for test_sentence in test_dataset.data:\n",
    "        predicted_sentence = [START_OUTPUT_INDEX]\n",
    "        \n",
    "        while (not predicted_sentence[-1] == END_OUTPUT_INDEX) and len(predicted_sentence) < DECODER_TIME_STEPS:\n",
    "            \n",
    "            next_token_index = len(predicted_sentence) - 1\n",
    "            \n",
    "            padded_predicted_sentence = pad_predicted_sentence(predicted_sentence)\n",
    "            feed_dict = {\n",
    "                model.dataset.data: test_sentence.reshape(1, -1), \n",
    "                model.dataset.labels: np.array(padded_predicted_sentence).reshape(1, -1)\n",
    "            }\n",
    "            next_token_index_predictions_for_all_timesteps = \\\n",
    "                session.run(model.predict_next_token(), feed_dict=feed_dict)\n",
    "            next_token_predictions = next_token_index_predictions_for_all_timesteps[next_token_index].ravel()\n",
    "            \n",
    "            next_token_prediction = np.argmax(np.random.multinomial(1, next_token_predictions))\n",
    "            if next_token_prediction not in [START_OUTPUT_INDEX, PAD_INDEX]:\n",
    "                predicted_sentence.append(next_token_prediction)\n",
    "    \n",
    "        print('Test Sentence: {}\\nPredicted Reversal: {}\\n'.format(\n",
    "                ''.join([index_to_token_lookup[index] for index in test_sentence]),\n",
    "                ''.join(index_to_token_lookup[index] for index in predicted_sentence[1:-1]),      \n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
